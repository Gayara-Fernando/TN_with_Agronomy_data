{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5fa6084-5582-412b-9f04-4fc66ca56c96",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bd9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import skimage as sky\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from scipy import ndimage\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f325c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices ('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ce7f7-e348-4f12-8a80-fc9090296ad3",
   "metadata": {},
   "source": [
    "Create the folds based on the stacked np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36350b45-d650-42b3-ba28-d48e1d06c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numpy files of the images and the sub image counts\n",
    "\n",
    "# data files\n",
    "all_train_sub_images = np.load(r\"/home/statsgeneral/gayara/Tasselnet/Filtered_90/Data_files/all_sub_windows_float16.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9465764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347113, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_sub_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d06d68-371e-4c82-b1f6-0d9a4187d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_sub_images_counts = np.load(r\"/home/statsgeneral/gayara/Tasselnet/Filtered_90/Data_files/all_sub_windows_counts_float16.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72452a65-c834-4280-a54a-f86792b7aac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347113,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_sub_images_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26624bf7-bcdf-4454-88d7-01c25a97e644",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.71 GiB for an array with shape (1347113, 32, 32, 3) and data type float16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m all_train_sub_images_counts \u001b[38;5;241m=\u001b[39m all_train_sub_images_counts\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m all_train_sub_images \u001b[38;5;241m=\u001b[39m \u001b[43mall_train_sub_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 7.71 GiB for an array with shape (1347113, 32, 32, 3) and data type float16"
     ]
    }
   ],
   "source": [
    "all_train_sub_images_counts = all_train_sub_images_counts.astype('float16')\n",
    "all_train_sub_images = all_train_sub_images.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3fdff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random sample based on a sequence\n",
    "\n",
    "# fix the seed\n",
    "random.seed(32)\n",
    "\n",
    "# all data\n",
    "rand_all_available = random.sample(range(len(all_train_sub_images)),len(all_train_sub_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37fefdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the 269423 data points and the corresponding counts\n",
    "train_np_arrays= [all_train_sub_images[index] for index in rand_all_available]\n",
    "train_sub_counts = [all_train_sub_images_counts[index] for index in rand_all_available]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f99836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the files all together\n",
    "data_final_np = np.stack(train_np_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e953a047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347113, 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a117f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_final_np = np.stack(train_sub_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34741fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347113,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_final_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74222136",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e418edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 269423  269424  269425 ... 1347110 1347111 1347112] TEST: [     0      1      2 ... 269420 269421 269422]\n",
      "TRAIN: [      0       1       2 ... 1347110 1347111 1347112] TEST: [269423 269424 269425 ... 538843 538844 538845]\n",
      "TRAIN: [      0       1       2 ... 1347110 1347111 1347112] TEST: [538846 538847 538848 ... 808266 808267 808268]\n",
      "TRAIN: [      0       1       2 ... 1347110 1347111 1347112] TEST: [ 808269  808270  808271 ... 1077688 1077689 1077690]\n",
      "TRAIN: [      0       1       2 ... 1077688 1077689 1077690] TEST: [1077691 1077692 1077693 ... 1347110 1347111 1347112]\n"
     ]
    }
   ],
   "source": [
    "X_train_all = []\n",
    "X_test_all = []\n",
    "y_train_all = []\n",
    "y_test_all = []\n",
    "for train_index, test_index in kf.split(data_final_np):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = data_final_np[train_index], data_final_np[test_index]\n",
    "    X_train_all.append(X_train)\n",
    "    X_test_all.append(X_test)\n",
    "    y_train, y_test = counts_final_np[train_index], counts_final_np[test_index]\n",
    "    y_train_all.append(y_train)\n",
    "    y_test_all.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9dc4ed-ab32-40df-81b0-da4445b816ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077691, 32, 32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f49ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store_path = r\"/work/statsgeneral/gayara/Tassel_datasets_F90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9d55884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 1\n",
    "# for data_set in X_train_all:\n",
    "#     data = data_set\n",
    "#     name = 'x_train_data_' + str(counter) \n",
    "#     np.save(Store_path + \"/\" + name, data)\n",
    "#     counter = counter + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59c26be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 1\n",
    "# for data_set in X_test_all:\n",
    "#     data_n = data_set\n",
    "#     name = 'x_test_data_' + str(counter) \n",
    "#     np.save(Store_path + \"/\" + name, data_n)\n",
    "#     counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5633b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 1\n",
    "# for data_set in y_train_all:\n",
    "#     data_n = data_set\n",
    "#     name = 'y_train_' + str(counter) \n",
    "#     np.save(Store_path + \"/\" + name, data_n)\n",
    "#     counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dce1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 1\n",
    "# for data_set in y_test_all:\n",
    "#     data_n = data_set\n",
    "#     name = 'y_test_' + str(counter) \n",
    "#     np.save(Store_path + \"/\" + name, data_n)\n",
    "#     counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0abdf4",
   "metadata": {},
   "source": [
    "Do model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59505aa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.3 GiB for an array with shape (1077690, 32, 32, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     es \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#     fit the frozen model:\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# # unfreeze few layers and retrain\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.9.1-py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.9.1-py39/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 12.3 GiB for an array with shape (1077690, 32, 32, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# need to introduce a counter for saving the models\n",
    "counter = 1\n",
    "for i in range(len(X_train_all)):\n",
    "    X_train = X_train_all[i]\n",
    "    y_train = y_train_all[i]\n",
    "    X_test = X_test_all[i]\n",
    "    y_test = y_test_all[i]\n",
    "    # load the pre-trained model on tassels\n",
    "    model = tf.keras.models.load_model('/home/statsgeneral/gayara/Tasselnet/Filtered_90/tasselnet1_overlapping_w32.h5')\n",
    "    # look at the input shape\n",
    "    model.input\n",
    "    \n",
    "    # need to remove some of the last layers\n",
    "    output_trial = model.layers[-8].output\n",
    "    \n",
    "# create the functional API model\n",
    "    reduced_model = tf.keras.models.Model(model.input, output_trial)\n",
    "\n",
    "# okay, now need to add back the dropout, the dense and activation\n",
    "\n",
    "# add dropout\n",
    "    added_dropout = tf.keras.layers.Dropout(0.5, name = \"New_dropout\")(model.layers[-8].output)\n",
    "\n",
    "# add flatten\n",
    "    added_flatten = tf.keras.layers.Flatten(name = \"Flatten2\")(added_dropout)\n",
    "\n",
    "# add dense\n",
    "    added_dense = tf.keras.layers.Dense(512, name = \"New_Dense\")(added_flatten)\n",
    "\n",
    "# add activation\n",
    "    added_Act = tf.keras.layers.Activation('relu', name = \"New_Activation\")(added_dense)\n",
    "\n",
    "# add dropout\n",
    "    added_dropout2 = tf.keras.layers.Dropout(0.5, name = \"New_dropout2\")(added_Act)\n",
    "\n",
    "# add dense\n",
    "    added_dense2 = tf.keras.layers.Dense(1, name = \"New_Dense2\")(added_dropout2)\n",
    "\n",
    "# add activation\n",
    "    added_Act2 = tf.keras.layers.Activation('relu', name = \"New_Activation2\")(added_dense2)\n",
    "    \n",
    "# define the new model with functional API\n",
    "    new_model = tf.keras.models.Model(model.input, added_Act2)\n",
    "    \n",
    "#     freeze layers\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "# compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    new_model.compile(loss='mean_squared_error', optimizer=opt, metrics = ['mean_absolute_error'])\n",
    "    \n",
    "# add early stopping\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights = True, verbose=1, patience=5)\n",
    "    \n",
    "#     fit the frozen model:\n",
    "    history = new_model.fit(X_train, y_train,\n",
    "          epochs = 50, callbacks = [es],\n",
    "          validation_data = (X_test, y_test), \n",
    "                       batch_size = 64, validation_batch_size = 64)\n",
    "    \n",
    "    \n",
    "# # unfreeze few layers and retrain\n",
    "    model.trainable = True\n",
    "\n",
    "    set_trainable = False\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'conv2d_3':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            \n",
    "# # compile the mdoel\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    new_model.compile(loss='mean_squared_error', optimizer=opt, metrics = ['mean_absolute_error'])\n",
    "    \n",
    "# # add early stopping\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights = True, verbose=1, patience=5)\n",
    "    \n",
    "# # fit the model (fine tuned)\n",
    "    history_new = new_model.fit(X_train, y_train,\n",
    "          epochs = 50, callbacks = [es],\n",
    "          validation_data = (X_test, y_test), \n",
    "                       batch_size = 2000, validation_batch_size = 2000)\n",
    "    \n",
    "    #     save the model\n",
    "    model_name = 'model_' + str(counter) + '.h5'\n",
    "    counter = counter + 1\n",
    "    new_model.save('/work/statsgeneral/gayara/tasselnet/models' + '/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae6a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.9 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.9-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
